# Tutorials

We present here a curated list of notebooks recommended to start with decomon,
available in the `tutorials/` folder of the repository.

```{contents}
---
depth: 1
local: true
---
```

## DECOMON tutorial #1

[![Github](https://img.shields.io/badge/see-Github-579aca?logo=github)](https://github.com/airbus/decomon/blob/main/tutorials/tutorial1_sinus-interactive.ipynb)
[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/airbus/decomon/blob/main/tutorials/tutorial1_sinus-interactive.ipynb)


_**Bounding the output of a Neural Network trained on a sinusoidal function**_

After training a model, we want to make sure that the model is *smooth*: it will predict almost the same output for any data "close" to the initial one, showing some robustness to perturbation. 

In this notebook, we train a Neural Network to approximate at best a simple sinusoidal function (the reference model). However, between test samples, we have no clue that the output of the Neural Network will look like. The objective is to have a formal proof that outputs of the neural network's predictions do not go to weird values. 

In the first part of the notebook, we define the reference function, build a training and test dataset and learn a dense fully connected neural network to approximate this reference function. 

In the second part of the notebook, we use *decomon* to compute guaranteed bounds to the output of the model.  

What we will show is how decomon module is able to provide guaranteed bounds that ensure our approximation will never have a strange behaviour between test dataset points. 

## DECOMON tutorial #2

[![Github](https://img.shields.io/badge/see-Github-579aca?logo=github)](https://github.com/airbus/decomon/blob/main/tutorials/tutorial2_noise_sensor.ipynb)
[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/airbus/decomon/blob/main/tutorials/tutorial2_noise_sensor.ipynb)

_**Local Robustness to sensor noise for Regression**_

Embedding simulation models developed during the design
of a platform opens a lot of potential new functionalities
but requires additional certification. Usually, these models require too much computing power, take too much time to run
so we need to build an approximation of these models that can
be compatible with operational constraints, hardware constraints, and real-time constraints. Also, we need to prove that
the decisions made by the system using the surrogate model
instead of the reference one will be safe.

A first assessment that can be performed is the **robustness of the prediction given sensor noise**: demonstrating that despite sensor noise, the neural network prediction remains consistent.

Local Robustness to **sensoir noise** can be performed efficiently thanks to formal robustness. In this notebook, we demonstrate how to derive deterministic upper and lower bounds of the output prediction of a neural network in the vicinity of a test sample.

## DECOMON tutorial #3

[![Github](https://img.shields.io/badge/see-Github-579aca?logo=github)](https://github.com/airbus/decomon/blob/main/tutorials/tutorial3_adversarial_attack.ipynb)
[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/airbus/decomon/blob/main/tutorials/tutorial3_adversarial_attack.ipynb)

_**Local Robustness to Adversarial Attacks for classification tasks**_

After training a model, we want to make sure that the model will give the same output for any images "close" to the initial one, showing some robustness to perturbation. 

In this notebook, we start from a classifier built on MNIST dataset that given a hand-written digit as input will predict the digit. This will be the first part of the notebook.

<img src="./data/Plot-of-a-Subset-of-Images-from-the-MNIST-Dataset.png" alt="examples of hand-written digit" width="600"/>

In the second part of the notebook, we will investigate the robustness of this model to unstructured modification of the input space: adversarial attacks. For this kind of attacks, **we vary the magnitude of the perturbation of the initial image** and want to assess that despite this noise, the classifier's prediction remain unchanged.

<img src="./data/illustration_adv_attacks.jpeg" alt="examples of hand-written digit" width="600"/>

What we will show is the use of decomon module to assess the robustness of the prediction towards noise.

## DECOMON tutorial #4

[![Github](https://img.shields.io/badge/see-Github-579aca?logo=github)](https://github.com/airbus/decomon/blob/main/tutorials/tutorial4_adversarial_brightness.ipynb)
[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/airbus/decomon/blob/main/tutorials/tutorial4_adversarial_brightness.ipynb)

_**Local Robustness to Brightness perturbations**_

After training a model, we want to make sure that the model will give the same output for any images "close" to the initial one, showing some robustness to perturbation. 

In this notebook, we start from a classifier built on MNIST dataset that given a hand-written digit as input will predict the digit. This will be the first part of the notebook.

<img src="./data/Plot-of-a-Subset-of-Images-from-the-MNIST-Dataset.png" alt="examples of hand-written digit" width="600"/>

In the second part of the notebook, we will investigate the robustness of this model to unstructured modification of the input space: adversarial attacks. For this kind of attacks, **we vary the magnitude of the perturbation of the initial image** and want to assess that despite this noise, the classifier's prediction remain unchanged.

<img src="./data/illustration_adv_attacks.jpeg" alt="examples of hand-written digit" width="600"/>

What we will show is the use of decomon module to assess the robustness of the prediction towards noise.


