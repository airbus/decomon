{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DECOMON tutorial #3 \n",
    "## Local Robustness to Adversarial Attacks for classification tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "After training a model, we want to make sure that the model will give the same output for any images \"close\" to the initial one, showing some robustness to perturbation. \n",
    "\n",
    "In this notebook, we start from a classifier built on MNIST dataset that given a hand-written digit as input will predict the digit. This will be the first part of the notebook.\n",
    "\n",
    "<img src=\"./data/Plot-of-a-Subset-of-Images-from-the-MNIST-Dataset.png\" alt=\"examples of hand-written digit\" width=\"600\"/>\n",
    "\n",
    "In the second part of the notebook, we will investigate the robustness of this model to unstructured modification of the input space: adversarial attacks. For this kind of attacks, **we vary the magnitude of the perturbation of the initial image** and want to assess that despite this noise, the classifier's prediction remain unchanged.\n",
    "\n",
    "<img src=\"./data/illustration_adv_attacks.jpeg\" alt=\"examples of hand-written digit\" width=\"600\"/>\n",
    "\n",
    "What we will show is the use of decomon module to assess the robustness of the prediction towards noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The notebook\n",
    "\n",
    "###Â imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "%matplotlib inline\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import ipywidgets as widgets\n",
    "import numpy as np\n",
    "from ipywidgets import interact\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "import os\n",
    "import os.path\n",
    "import pickle as pkl\n",
    "import time\n",
    "from contextlib import closing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decomon.wrapper import refine_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_min = np.ones((3, 4, 5))\n",
    "x_max = 2 * x_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refine_boxes(x_min, x_max, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load images\n",
    "\n",
    "We load MNIST data from keras datasets. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ara\n",
    "img_rows, img_cols = 28, 28\n",
    "(x_train, y_train_), (x_test, y_test_) = mnist.load_data()\n",
    "x_train = x_train.reshape((-1, 784))\n",
    "x_test = x_test.reshape((-1, 784))\n",
    "x_train = x_train.astype(\"float32\")\n",
    "x_test = x_test.astype(\"float32\")\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "y_train = keras.utils.to_categorical(y_train_)\n",
    "y_test = keras.utils.to_categorical(y_test_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### learn the model (classifier for MNIST images)\n",
    "\n",
    "For the model, we use a small fully connected network. It is made of 6 layers with 100 units each and ReLU activation functions. **Decomon** is compatible with a large set of Keras layers, so do not hesitate to modify the architecture.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(100, activation=\"relu\", input_dim=784))\n",
    "model.add(Dense(100, activation=\"relu\"))\n",
    "model.add(Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\"adam\", \"categorical_crossentropy\", metrics=\"acc\")\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=32, shuffle=True, validation_split=0.2, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x_test, y_test, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, we see that the assessment of performance of the model on data that was not seen during training shows pretty good results: around 0.97 (maximum value is 1). It means that out of 100 images, the model was able to guess the correct digit for 97 images. But how can we guarantee that we will get this performance for images different from the ones in the test dataset? \n",
    "\n",
    "- If we perturbate a \"little\" an image that was well predicted, will the model stay correct? \n",
    "- Up to which perturbation?  \n",
    "- Can we guarantee that the model will output the same digit for a given perturbation? \n",
    "\n",
    "This is where decomon comes in. \n",
    "\n",
    "<img src=\"./data/decomon.jpg\" alt=\"Decomon!\" width=\"400\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Decomon for Local Robustness to misclassification\n",
    "\n",
    "In this section, we detail how to prove local robustness to misclassification. Misclassification can be studied with the global optimisation of a function f:\n",
    "\n",
    "$$ f(x; \\Omega) = \\max_{z\\in \\Omega} \\text{NN}_{j\\not= i}(z) - \\text{NN}_i(z)\\;\\; \\text{s.t}\\;\\; i = argmax\\;\\text{NN}(x)$$\n",
    "\n",
    "If the maximum of f is **negative**, this means that whathever the input sample from the domain, the value outputs by the neural network NN for class i will always be greater than the value output for another class. Hence, there will be no misclassification possible. This is **adversarial robustness**.\n",
    "\n",
    "<img src=\"./data/tuto_3_formal_robustness.png\" alt=\"Decomon!\" width=\"400\"/>\n",
    "\n",
    "In that order, we will use the [decomon](https://gheprivate.intra.corp/CRT-DataScience/decomon/tree/master/decomon) library. Decomon combines several optimization trick, including linear relaxation\n",
    "to get state-of-the-art outer approximation.\n",
    "\n",
    "To use **decomon** for **adversarial robustness** we first need the following imports:\n",
    "+ *from decomon.models import convert*: to convert our current Keras model into another neural network nn_model. nn_model will output the same prediction that our model and adds extra information that will be used to derive our formal bounds. For a sake of clarity, how to get such bounds is hidden to the user\n",
    "\n",
    "+ *from decomon import get_adv_box*: a genereric method to get an upper bound of the funtion f described previously. If the returned value is negative, then we formally assess the robustness to misclassification.\n",
    "\n",
    "+ *from decomon import check_adv_box*: a generic method that computes the maximum of a lower bound of f. Eventually if this value is positive, it demonstrates that the function f takes positive value. It results that a positive value formally proves the existence of misclassification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decomon import check_adv_box, get_adv_box\n",
    "from decomon.models import convert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For computational efficiency, we convert the model into its decomon version once and for all.\n",
    "Note that the decomon method will work on the non-converted model. To obtain more refined guarantees, we activate an option denoted **forward**. You can speed up the method by removing this option in the convert method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decomon_model = convert(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decomon import build_formal_adv_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_model = build_formal_adv_model(decomon_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ = x_train[:1]\n",
    "eps = 1e-2\n",
    "z = np.concatenate([x_[:, None] - eps, x_[:, None] + eps], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_adv_box(decomon_model, x_, x_, source_labels=y_train[0].argmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_model.predict([x_, z, y_train[:1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute gradient\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tensor = tf.convert_to_tensor(x_, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape() as t:\n",
    "    t.watch(x_tensor)\n",
    "    z_tensor = Concatenate(1)([x_tensor[:, None] - eps, x_tensor[:, None] + eps])\n",
    "    output = adv_model([x_, z_tensor, y_train[:1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = output\n",
    "gradients = t.gradient(output, x_tensor)\n",
    "mask = gradients.numpy()\n",
    "# scale between 0 and 1.\n",
    "mask = mask - mask.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(gradients.numpy().reshape((28, 28)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_mask = np.zeros((784,))\n",
    "img_mask[np.argsort(mask[0])[::-1][:100]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img_mask.reshape((28, 28)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(mask.reshape((28, 28)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(x_.reshape((28, 28)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We offer an interactive visualisation of the basic adversarial robustness method from decomon **get_adv_upper**. We randomly choose 10 test images use **get_adv_upper** to assess their robustness to misclassification pixel perturbations. The magnitude of the noise on each pixel is independent and bounded by the value of the variable epsilon. The user can reset the examples and vary the noise amplitude.\n",
    "\n",
    "Note one of the main advantage of decomon: **we can assess robustness on batches of data!**\n",
    "\n",
    "Circled in <span style=\"color:green\">green</span> are examples that are formally assessed to be robust, <span style=\"color:orange\">orange</span> examples that could be robust and  <span style=\"color:red\">red</span> examples that are formally non robust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frame(epsilon, reset=0, filename=\"./data/.hidden_index.pkl\"):\n",
    "    n_cols = 5\n",
    "    n_rows = 2\n",
    "    n_samples = n_cols * n_rows\n",
    "    if reset:\n",
    "        index = np.random.permutation(len(x_test))[:n_samples]\n",
    "\n",
    "        with closing(open(filename, \"wb\")) as f:\n",
    "            pkl.dump(index, f)\n",
    "        # save data\n",
    "    else:\n",
    "        # check that file exists\n",
    "\n",
    "        if os.path.isfile(filename):\n",
    "            with closing(open(filename, \"rb\")) as f:\n",
    "                index = pkl.load(f)\n",
    "        else:\n",
    "            index = np.arange(n_samples)\n",
    "            with closing(open(filename, \"wb\")) as f:\n",
    "                pkl.dump(index, f)\n",
    "    # x = np.concatenate([x_test[0:1]]*10, 0)\n",
    "    x = x_test[index]\n",
    "\n",
    "    x_min = np.maximum(x - epsilon, 0)\n",
    "    x_max = np.minimum(x + epsilon, 1)\n",
    "\n",
    "    n_cols = 5\n",
    "    n_rows = 2\n",
    "    fig, axs = plt.subplots(n_rows, n_cols)\n",
    "\n",
    "    fig.set_figheight(n_rows * fig.get_figheight())\n",
    "    fig.set_figwidth(n_cols * fig.get_figwidth())\n",
    "    plt.subplots_adjust(hspace=0.2)  # increase vertical separation\n",
    "    axs_seq = axs.ravel()\n",
    "\n",
    "    source_label = np.argmax(model.predict(x), 1)\n",
    "    start_time = time.process_time()\n",
    "    upper = get_adv_box(decomon_model, x_min, x_max, source_labels=source_label)\n",
    "    lower = check_adv_box(decomon_model, x_min, x_max, source_labels=source_label)\n",
    "    end_time = time.process_time()\n",
    "    count = 0\n",
    "    time.sleep(1)\n",
    "    r_time = \"{:.2f}\".format(end_time - start_time)\n",
    "    fig.suptitle(\n",
    "        \"Formal Robustness to Adversarial Examples with eps={} running in {} seconds\".format(epsilon, r_time),\n",
    "        fontsize=16,\n",
    "    )\n",
    "    for i in range(n_cols):\n",
    "        for j in range(n_rows):\n",
    "\n",
    "            ax = axs[j, i]\n",
    "            ax.imshow(x[count].reshape((28, 28)), cmap=\"Greys\")\n",
    "            robust = \"ROBUST\"\n",
    "            if lower[count] >= 0:\n",
    "                color = \"red\"\n",
    "                robust = \"NON ROBUST\"\n",
    "            elif upper[count] < 0:\n",
    "                color = \"green\"\n",
    "            else:\n",
    "                color = \"orange\"\n",
    "                robust = \"MAYBE ROBUST\"\n",
    "\n",
    "            ax.get_xaxis().set_visible(False)\n",
    "            ax.get_yaxis().set_visible(False)\n",
    "            # Create a Rectangle patch\n",
    "            rect = patches.Rectangle((0, 0), 27, 27, linewidth=3, edgecolor=color, facecolor=\"none\")\n",
    "            ax.add_patch(rect)\n",
    "            ax.set_title(robust)\n",
    "            count += 1\n",
    "\n",
    "\n",
    "interact(\n",
    "    frame,\n",
    "    epsilon=widgets.FloatSlider(\n",
    "        value=0.0,\n",
    "        min=0.0,\n",
    "        max=5.0 / 255.0,\n",
    "        step=0.0001,\n",
    "        continuous_update=False,\n",
    "        readout_format=\".4f\",\n",
    "    ),\n",
    "    reset=widgets.IntSlider(value=0.0, min=0, max=1, step=1, continuous_update=False),\n",
    "    fast=widgets.IntSlider(value=1.0, min=0, max=1, step=1, continuous_update=False),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As explained previously, the method **get_adv_upper** output a constant upper bound that is valid on the whole domain.\n",
    "Sometimes, this bound can be too lose and needs to be refined by splitting the input domain into sub domains.\n",
    "Several heuristics are possible and you are free to develop your own or take an existing one of the shelf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
