{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # DECOMON tutorial #1\n",
    " \n",
    "_**Bounding the output of a Neural Network trained on a sinusoidal function**_\n",
    "\n",
    "<decomonlinks>\n",
    "<p align=\"center\">\n",
    "  <img src=\"data/decomon.jpg\" alt=\"Decomon!\" width=\"100\">\n",
    "</p>\n",
    "\n",
    "\n",
    "\n",
    "- &#x1F4DA; <a href=\"https://airbus.github.io/decomon\"> Documentation </a>\n",
    "- <a href=\"https://github.com/airbus/decomon\"> Github </a>\n",
    "- <a href=\"https://airbus.github.io/decomon/main/tutorials.html \"> Tutorials </a>\n",
    "    \n",
    "_Author: [Melanie DUCOFFE](https://fr.linkedin.com/in/m%C3%A9lanie-ducoffe-bbb53165)_\n",
    "</decomonlinks>\n",
    "\n",
    "After training a model, we want to make sure that the model is *smooth*: it will predict almost the same output for any data \"close\" to the initial one, showing some robustness to perturbation. \n",
    "\n",
    "In this notebook, we train a Neural Network to approximate at best a simple sinusoidal function (the reference model). However, between test samples, we have no clue that the output of the Neural Network will look like. The objective is to have a formal proof that outputs of the neural network's predictions do not go to weird values. \n",
    "\n",
    "In the first part of the notebook, we define the reference function, build a training and test dataset and learn a dense fully connected neural network to approximate this reference function. \n",
    "\n",
    "In the second part of the notebook, we use *decomon* to compute guaranteed bounds to the output of the model.  \n",
    "\n",
    "What we will show is how decomon module is able to provide guaranteed bounds that ensure our approximation will never have a strange behaviour between test dataset points. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- When running this notebook on Colab, we need to install *decomon* if on Colab. \n",
    "- If you run this notebook locally, do it inside the environment in which you [installed *decomon*](https://airbus.github.io/decomon/main/install.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On Colab: install the library\n",
    "on_colab = \"google.colab\" in str(get_ipython())\n",
    "if on_colab:\n",
    "    import sys  # noqa: avoid having this import removed by pycln\n",
    "\n",
    "    # install dev version for dev doc, or release version for release doc\n",
    "    !{sys.executable} -m pip install -U pip\n",
    "    !{sys.executable} -m pip install git+https://github.com/airbus/decomon@keras3#egg=decomon\n",
    "    !{sys.executable} -m pip uninstall -y keras keras-nightly\n",
    "    !{sys.executable} -m pip install keras-nightly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow\n",
    "\n",
    "%matplotlib inline\n",
    "import ipywidgets as widgets\n",
    "import numpy as np\n",
    "from ipywidgets import interact\n",
    "from tensorflow.keras.layers import Activation, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "print(\"Notebook run using tensorflow:\", tensorflow.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and Train a Neural Network on a sinusoide\n",
    "\n",
    "The sinusoide funtion is defined on a $[-1 ; 1 ]$ interval. We put a factor in the sinusoide to have several periods of oscillations. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-1, 1, 1000)\n",
    "y = np.sin(10 * x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We approximate this function by a fully connected network composed of 4 hidden layers of size 100, 100, 20 and 20 respectively. Rectified Linear Units (ReLU) are chosen as activation functions for all the neurons. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = []\n",
    "layers.append(Dense(100, activation=\"linear\", input_dim=1))  # specify the dimension of the input space\n",
    "layers.append(Activation(\"relu\"))\n",
    "layers.append(Dense(100, activation=\"linear\"))\n",
    "layers.append(Activation(\"relu\"))\n",
    "layers.append(Dense(20, activation=\"linear\"))\n",
    "layers.append(Activation(\"relu\"))\n",
    "layers.append(Dense(20, activation=\"linear\"))\n",
    "layers.append(Activation(\"relu\"))\n",
    "layers.append(Dense(1, activation=\"linear\"))\n",
    "model = Sequential(layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we specify the optimization method and the metric, in this case a classical Means Square Error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\"adam\", \"mse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we train the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x, y, batch_size=32, shuffle=True, epochs=100, verbose=0)\n",
    "# verbose=0 removes the printing along the training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can now display the output of our neural network on a more refined grid. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.linspace(-1, 1, 2000)\n",
    "y_pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_test, y_pred)\n",
    "plt.legend([\"NN\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks reasonably as a sinus function and nothing crazy is happening. Still, are we able to ensure that when testing on a finer grid, there will not be a point where the learnt model will not infer a crazy output? The answer is in general no, we cannot. And this is more and more true when the complexity of the model is increasing.\n",
    "\n",
    "Shallow networks are more likely to be part of no human intervention system. They require our full attention and our best efforts to ensure safety. Usually we tend to *trust* the prediction of neural network based solely on the prediction on the test dataset. However, with a tighter granularity or using verification methods, we would actually realize that the neural network outputs an unintended peak. We highlight this unintended behaviour in the next figure\n",
    "    \n",
    "   <img src=\"./data/sinusoide_fail.png\" alt=\"sinusoide_fail\" width=\"400\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately, decomon allows to find rigorous bounds of the model that will be true on the full input domain definition. \n",
    "\n",
    "<img src=\"data/decomon.jpg\" alt=\"Decomon!\" width=\"400\"/>\n",
    "\n",
    "## Applying decomon\n",
    "\n",
    "First *decomon* requires specific imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decomon import get_lower_box, get_range_box, get_upper_box\n",
    "from decomon.models import clone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can compute global constant upper and lower bounds that hold for the whole interval [-1, 1] To use decomon, we need to convert it into another Keras Model that contains extra information necessary to compute upper and lower bounds on a domain. Without setting specific options, **decomon_model** will share the parameters (a.k.a weights and biases) of our model.\n",
    "\n",
    "Several methods exist and we refer you to [Automatic Perturbation Analysis for\n",
    "Scalable Certified Robustness and Beyond](https://proceedings.neurips.cc/paper/2020/file/0cbc5671ae26f67871cb914d81ef8fc1-Paper.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert our model into a decomon model:\n",
    "decomon_model = clone(model, method=\"crown\")  # method is optionnal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several methods are possible among this list: **forward-ibp, forward-affine, forward-hybrid, crown-forward-ibp, crown-forward-affine, crown-forward-hybrid, crown**.\n",
    "\n",
    "Depending on the inner recipes, those methods will output different informations:\n",
    "\n",
    "- **forward-ibp, crown-forward-ibp, crown**: constant upper and lower bounds that outer-approximate every output neurons\n",
    "- **forward-affine, crown-forward-affine**: affine upper bound and affine lower bound given the input of the network that outer-approximate every output neurons\n",
    "- **forward-hybrid, crown-forward-hybrid**: both types of bounds\n",
    "    \n",
    "By default, the convert method converts the Keras model in **crown**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.process_time()\n",
    "X_min = np.array([x.min()])\n",
    "X_max = np.array([x.max()])\n",
    "\n",
    "global_upper = get_upper_box(decomon_model, X_min, X_max)\n",
    "global_lower = get_lower_box(decomon_model, X_min, X_max)\n",
    "end_time = time.process_time()\n",
    "print(end_time - start_time)\n",
    "\n",
    "plt.plot(x_test, y_pred)\n",
    "plt.plot(x_test, global_upper * np.ones_like(y_pred))\n",
    "plt.plot(x_test, global_lower * np.ones_like(y_pred))\n",
    "plt.legend([\"NN\", \"global upper\", \"global lower\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a tighter region, the bounds became tighter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can play with the interval [x_min, x_max]\n",
    "x_min = -0.25\n",
    "x_max = 0.25\n",
    "X_min = np.array([x_min])\n",
    "X_max = np.array([x_max])\n",
    "\n",
    "refine_upper, refine_lower = get_range_box(decomon_model, X_min, X_max)\n",
    "\n",
    "print(\n",
    "    \"if {}<= x <= {}, we can ensure that {} <= NN(x) <= {}\".format(x_min, x_max, refine_lower.min(), refine_upper.min())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(min_value=-1.0, max_value=1.0, n_step=300):\n",
    "    if min_value > max_value:\n",
    "        print(\"Error min_value={} >max_value={}. Reset the demo to the whole domain\".format(min_value, max_value))\n",
    "        min_value = -1\n",
    "        max_value = 1.0\n",
    "    samples = np.linspace(min_value, max_value, n_step)\n",
    "\n",
    "    boxes = np.zeros((len(samples[:-1]), 2, 1))\n",
    "    boxes[:, 0, 0] = samples[:-1]\n",
    "    boxes[:, 1, 0] = samples[1:]\n",
    "\n",
    "    upper = get_upper_box(decomon_model, samples[:-1], samples[1:])\n",
    "    lower = get_lower_box(decomon_model, samples[:-1], samples[1:])\n",
    "\n",
    "    y_samples = model.predict(samples, verbose=0)\n",
    "\n",
    "    x_ = []\n",
    "    upper_ = []\n",
    "    lower_ = []\n",
    "\n",
    "    # lower and upper bounds are step function\n",
    "    for i in range(n_step - 1):\n",
    "        x_.append(samples[i])\n",
    "        x_.append(samples[i + 1])\n",
    "        upper_.append(upper[i])\n",
    "        upper_.append(upper[i])\n",
    "        lower_.append(lower[i])\n",
    "        lower_.append(lower[i])\n",
    "\n",
    "    ##########\n",
    "    n_cols = 2\n",
    "    fig, axs = plt.subplots(n_cols)\n",
    "    fig.set_figheight(n_cols * fig.get_figheight())\n",
    "    fig.set_figwidth(n_cols * fig.get_figwidth())\n",
    "    plt.subplots_adjust(hspace=0.2)  # increase vertical separation\n",
    "    axs_seq = axs.ravel()\n",
    "\n",
    "    ax = axs[0]\n",
    "    ax.plot(x_test, y_pred)\n",
    "    ax.plot(samples, y_samples, \"x\", c=\"k\")\n",
    "    ax.legend([\"NN\"])\n",
    "    ax.set_title(\"Sampling n_step={}\".format(n_step))\n",
    "\n",
    "    ax = axs[1]\n",
    "    ax.plot(x_test, y_pred)\n",
    "    ax.plot(samples, y_samples, \"x\", c=\"k\")\n",
    "    ax.plot(samples, y_samples)\n",
    "    ax.plot(x_, upper_, c=\"purple\")\n",
    "    ax.plot(x_, lower_, c=\"orange\")\n",
    "    ax.legend([\"NN\", \"test\", \"interpolation\", \"upper\", \"lower\"])\n",
    "    ax.set_title(\"Bounding the prediction of the neural network\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interact(\n",
    "    func,\n",
    "    n_step=widgets.IntSlider(value=0, min=10, max=100, step=1, continuous_update=False),\n",
    "    min_value=widgets.FloatSlider(value=-0.3, min=-1, max=1.0, step=0.01, continuous_update=False),\n",
    "    max_value=widgets.FloatSlider(value=0.3, min=-1, max=1, step=0.01, continuous_update=False),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous interactive graph, you can set:\n",
    "- n_step: the number of intervals on which we divide the input space to improve the bounds deduced from *decomon*\n",
    "- min_value: the minimun value of the interval\n",
    "- max_value: the maximum value of the interval\n",
    "\n",
    "The graphs below show you:\n",
    "- the trained model with sample values drawn with the n_step discretization of the $[min_{value} ; max_{value}]$ interval\n",
    "- the increasing and decreasing function on each local interval and in dashed the increasing and decreasing function computed on the whole interval\n",
    "- the bounds computed with *decomon* on the n_step intervals\n",
    "\n",
    "Increasing the number of n_step allows to have tighter bounds. Hopefully, this number will keep reasonable for your preferred model!\n",
    "\n",
    "Enjoy!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
