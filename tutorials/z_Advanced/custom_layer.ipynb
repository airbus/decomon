{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed5852cf-7c70-440a-b8ae-558f9e792ec5",
   "metadata": {},
   "source": [
    "# How to implement its own custom decomon layer?\n",
    "\n",
    "<decomonlinks>\n",
    "<p align=\"center\">\n",
    "  <img src=\"data/decomon.jpg\" alt=\"Decomon!\" width=\"100\">\n",
    "</p>\n",
    "\n",
    "\n",
    "\n",
    "- &#x1F4DA; <a href=\"https://airbus.github.io/decomon\"> Documentation </a>\n",
    "- <a href=\"https://github.com/airbus/decomon\"> Github </a>\n",
    "- <a href=\"https://airbus.github.io/decomon/main/tutorials.html \"> Tutorials </a>\n",
    "    \n",
    "_Author: [Melanie DUCOFFE](https://fr.linkedin.com/in/m%C3%A9lanie-ducoffe-bbb53165)_\n",
    "</decomonlinks>\n",
    "\n",
    "When using decomon on customized Keras layers (or not already implemented in decomon), one has to implement their decomon counterpart. \n",
    "The easiest way is to simply implement their constant and affine relaxation, as explained in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5478238-ebf0-4760-af96-60d78c4f1f38",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae44949-26f2-4eb8-bcfd-2bd28c16f2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import keras\n",
    "import keras.ops as K\n",
    "import numpy as np\n",
    "from keras.layers import Dense, Input, Layer\n",
    "from keras.models import Sequential\n",
    "\n",
    "# from decomon import get_lower_box, get_upper_box\n",
    "from decomon import clone\n",
    "from decomon.keras_utils import batch_multid_dot\n",
    "from decomon.layers import DecomonLayer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a695903-0d16-4fa8-be3a-d868b9884b1a",
   "metadata": {},
   "source": [
    "## Custom keras layer and keras model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d5523e-23a3-4c36-82ca-ef65a20b1e03",
   "metadata": {},
   "source": [
    "We implement here 2 keras layers:\n",
    "- a linear layer: simply doubling its input\n",
    "- a non-linear layer: squaring its input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ba665b-20d0-48c4-8a69-5723136aa0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Double(Layer):\n",
    "    \"\"\"Doubling layer.\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"Take double.\"\"\"\n",
    "        return inputs * 2\n",
    "\n",
    "\n",
    "class Square(Layer):\n",
    "    \"\"\"Square layer.\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"Take square.\"\"\"\n",
    "        return inputs**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc24f31a-bfba-432f-8e51-93e196096d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([Input((2,)), Double(), Dense(10), Square()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c18d606-ae77-4616-b914-f52fddb7994e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[2].input.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4811e9-cd61-4e37-aed5-608188a7b067",
   "metadata": {},
   "source": [
    "## Decomon custom layer implementation\n",
    "\n",
    "We need to derive from `DecomonLayer` and implement some methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe939f13-8f8c-4a4e-916e-383d0ae641cf",
   "metadata": {},
   "source": [
    "### Linear layer\n",
    "\n",
    "For the linear layer, we only have to give the proper affine representation of the layer (with proper shape), once we have specified that it is a linear layer. \n",
    "As we have a linear (or affine) layer, this representation is independent of the batch and will be given as such. \n",
    "\n",
    "More precisely, we need to return weights and bias `w` and `b` such that\n",
    "\n",
    "    layer(x) = x * w + b\n",
    "\n",
    "where the multiplication is actually `keras.ops.tensordot` on all non-batch axes of `x` and the the first non-batch ones of `w`, same number as the number of non-batch axes in `x`. \n",
    "\n",
    "This can performed via `batch_multi_dot()` from decomon (same function will be used for non-linear case).\n",
    "\n",
    "In the generic case, with 1 output, the shapes are:\n",
    "    \n",
    "    - x ~ (batchsize,) + layer.input.shape[1:]\n",
    "    - b ~ layer.output.shape[1:]\n",
    "    - w ~ layer.input.shape[1:] + layer.output.shape[1:]\n",
    "\n",
    "We can also use *diagonal representation*: this means that `w` is represented only by its diagonal. (This only possible if input and output are of same shape). The shapes are:\n",
    "\n",
    "    - x ~ (batchsize,) + layer.input.shape[1:]\n",
    "    - b ~ layer.output.shape[1:]\n",
    "    - w ~ layer.output.shape[1:]\n",
    "\n",
    "    - layer.input.shape[1:] == layer.output.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c519f84-de7b-40e6-8ee2-8240e4a3a0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecomonDouble(DecomonLayer):\n",
    "    linear = True  # specifying that this is a linear layer\n",
    "    diagonal = True  # specifying `w` is represented by its diagonal\n",
    "\n",
    "    def get_affine_representation(self):\n",
    "        bias_shape = self.layer.input.shape[\n",
    "            1:\n",
    "        ]  # a decomon layer has always an attribute `layer` which is the corresponding keras layer for which it is the decomon counterpart.\n",
    "        w = 2 * keras.ops.ones(bias_shape)\n",
    "        b = keras.ops.zeros(bias_shape)\n",
    "        return w, b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c7bfe2-4ff4-471e-a7f3-007d9309b9ba",
   "metadata": {},
   "source": [
    "#### Verification\n",
    "\n",
    "Let us check the affine representation by comparing it with actual output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4735f8-1f6f-46b7-ace6-c4ef69287bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the keras layer and build it\n",
    "layer = Double()\n",
    "layer(Input((2,)))\n",
    "\n",
    "# Instantiate the corresponding decomon layer\n",
    "decomon_layer = DecomonDouble(layer=layer)\n",
    "\n",
    "# Keras input/output\n",
    "x = K.convert_to_tensor(np.random.random((3, 2)), dtype=keras.config.floatx())  # ensure using same precision as default\n",
    "layer_output_np = K.convert_to_numpy(layer(x))\n",
    "\n",
    "# Recompute with affine representation\n",
    "w, b = decomon_layer.get_affine_representation()\n",
    "missing_batchsize = (False, True)  # specify that `w` is missing batchsize (but not x)\n",
    "diagonal = (False, True)  # specify that `w` is represented by its diagonal\n",
    "recomputed_layer_output = batch_multid_dot(x, w, missing_batchsize=missing_batchsize, diagonal=diagonal) + b\n",
    "recomputed_layer_output_np = K.convert_to_numpy(recomputed_layer_output)\n",
    "\n",
    "# Compare\n",
    "np.testing.assert_almost_equal(recomputed_layer_output_np, layer_output_np)\n",
    "\n",
    "print(\"Perfect!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2bb4cb1-7548-4344-a007-90091301608e",
   "metadata": {},
   "source": [
    "### Non-linear layer\n",
    "\n",
    "For the non-linear layer, we need to give the constant and affine relaxation of the layer (with proper shape)\n",
    "\n",
    "#### Constant relaxation\n",
    "\n",
    "Given lower and upper bounds on layer input, we give lower and upper constant bounds on layer output (with a batchsize).\n",
    "\n",
    "#### Affine relaxation\n",
    ". \n",
    "Given lower and upper bounds on layer input, we need to return weights and biases `w_l`, `b_l`, `w_u`, and `b_u` such that\n",
    "\n",
    "    x * w_l + b_l <= layer(x) <= x * w_u + b_u\n",
    "\n",
    "where the multiplication is batch-wise, and on multiple axes (the first non-batch ones of `w`, same number as the number of non-batch axes in `x`). This is performed via `batch_multi_dot()` from decomon.\n",
    "\n",
    "In the generic case, with 1 output, the shape are:\n",
    "    \n",
    "    - x ~ (batchsize,) + layer.input.shape[1:]\n",
    "    - b_l, b_l ~ (batchsize,) + layer.output.shape[1:]\n",
    "    - w_l, w_u ~ (batchsize,) + layer.input.shape[1:] + layer.output.shape[1:]\n",
    "\n",
    "\n",
    "We can also use *diagonal representation*: as before, this means that `w_l` and `w_u` will  be represented by their diagonal, so of the same shape as  `b_l` and `b_u`. Only possible if input and output of the layer share the same shape.\n",
    "\n",
    "    - x ~ (batchsize,) + layer.input.shape[1:]\n",
    "    - b_l, b_l ~ (batchsize,) + layer.output.shape[1:]\n",
    "    - w_l, w_u ~ (batchsize,) + layer.output.shape[1:]\n",
    "\n",
    "    - layer.input.shape[1:] == layer.output.shape[1:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4c9c28-712b-424d-9a7e-238890dca400",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecomonSquare(DecomonLayer):\n",
    "    diagonal = True  # specifying `w_l` and `w_u` are represented by their diagonal\n",
    "\n",
    "    def forward_ibp_propagate(self, lower, upper):\n",
    "        # image of bounds\n",
    "        f_lower = lower**2\n",
    "        f_upper = upper**2\n",
    "\n",
    "        # lower bound: if same sign, the minimum between 2 images, if opposite signs, 0.\n",
    "        lower_out = K.where(K.sign(lower * upper) > 0, K.minimum(f_lower, f_upper), 0.0)\n",
    "        # upper_bound: the maximum between 2 images\n",
    "        upper_out = K.maximum(f_lower, f_upper)\n",
    "\n",
    "        return lower_out, upper_out\n",
    "\n",
    "    def get_affine_bounds(self, lower, upper):\n",
    "        # image of bounds\n",
    "        f_lower = lower**2\n",
    "        f_upper = upper**2\n",
    "\n",
    "        # lower bound:\n",
    "        # - opposite signs: 0 hyperplan\n",
    "        # - same signs: tangent hyperplan at minimum point\n",
    "        w_l = K.where(\n",
    "            K.sign(lower * upper) > 0,\n",
    "            K.where(\n",
    "                lower < 0,\n",
    "                2 * upper,\n",
    "                2 * lower,\n",
    "            ),\n",
    "            0.0,\n",
    "        )\n",
    "        b_l = K.where(\n",
    "            K.sign(lower * upper) > 0,\n",
    "            K.where(\n",
    "                lower < 0,\n",
    "                -(upper**2),\n",
    "                -(lower**2),\n",
    "            ),\n",
    "            0.0,\n",
    "        )\n",
    "\n",
    "        # upper bound: by convexity, hyperplan between lower and upper\n",
    "        w_u = (f_upper - f_lower) / K.maximum(\n",
    "            K.cast(keras.config.epsilon(), dtype=upper.dtype), upper - lower\n",
    "        )  # avoid dividing by 0. -> replace by epsilon.\n",
    "        b_u = f_lower - w_u * lower\n",
    "\n",
    "        return w_l, b_l, w_u, b_u"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702a789d-2f83-44c3-b1bf-d11b71902a11",
   "metadata": {},
   "source": [
    "#### Verification\n",
    "\n",
    "Let us check the relaxations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a7b648-426b-4481-9dcb-d6512d1e8d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the keras layer and build it\n",
    "layer = Square()\n",
    "layer(Input((2,)))\n",
    "\n",
    "# Instantiate the corresponding decomon layer\n",
    "decomon_layer = DecomonSquare(layer=layer)\n",
    "\n",
    "# Keras bounds/input/output\n",
    "lowers = [-2, -1, 1]\n",
    "uppers = [-1, 1, 2]\n",
    "\n",
    "x_np = np.concatenate([np.random.random((1, 2)) * (u - l) + l for l, u in zip(lowers, uppers)], axis=0)\n",
    "lower_np = np.concatenate([np.reshape([l, l], (1, 2)) for l in lowers], axis=0)\n",
    "upper_np = np.concatenate([np.reshape([u, u], (1, 2)) for u in uppers], axis=0)\n",
    "\n",
    "x = K.convert_to_tensor(x_np, dtype=keras.config.floatx())  # ensure using same precision as default\n",
    "lower = K.convert_to_tensor(lower_np, dtype=keras.config.floatx())\n",
    "upper = K.convert_to_tensor(upper_np, dtype=keras.config.floatx())\n",
    "\n",
    "layer_output_np = K.convert_to_numpy(layer(x))\n",
    "\n",
    "\n",
    "# constant bounds\n",
    "lower_ibp, upper_ibp = decomon_layer.forward_ibp_propagate(lower, upper)\n",
    "\n",
    "# affine bounds => computed constant bounds\n",
    "w_l, b_l, w_u, b_u = decomon_layer.get_affine_bounds(lower, upper)\n",
    "diagonal = (False, True)  # specify that `w` is represented by its diagonal\n",
    "lower_affine = batch_multid_dot(x, w_l, diagonal=diagonal) + b_l\n",
    "upper_affine = batch_multid_dot(x, w_u, diagonal=diagonal) + b_u\n",
    "\n",
    "\n",
    "lower_affine_np = K.convert_to_numpy(lower_affine)\n",
    "upper_affine_np = K.convert_to_numpy(upper_affine)\n",
    "lower_ibp_np = K.convert_to_numpy(lower_ibp)\n",
    "upper_ibp_np = K.convert_to_numpy(upper_ibp)\n",
    "\n",
    "\n",
    "# comparison\n",
    "assert (lower_affine_np <= layer_output_np).all()\n",
    "assert (upper_affine_np >= layer_output_np).all()\n",
    "assert (lower_ibp_np <= layer_output_np).all()\n",
    "assert (upper_ibp_np >= layer_output_np).all()\n",
    "\n",
    "print(\"Perfect!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f5b684-34d9-40e2-a5e3-718410e0e65d",
   "metadata": {},
   "source": [
    "## Convert to decomon model\n",
    "\n",
    "We need to specify the mapping between keras and decomon custom layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd201a5-a066-4690-9068-9c435da76a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "decomon_model = clone(\n",
    "    model,\n",
    "    mapping_keras2decomon_classes={Square: DecomonSquare, Double: DecomonDouble},  # custom layers mapping\n",
    "    final_ibp=True,  # keep final constant bounds\n",
    "    final_affine=False,  # drop final affine bounds\n",
    ")\n",
    "\n",
    "decomon_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9f79b2-cc58-4f28-9da2-a741b736a7be",
   "metadata": {},
   "source": [
    "Get formal lower and upper bounds on a box domain [0,1] for inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8970932a-fafb-4ef0-8dcf-516ac220639f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a fake box with the right shape\n",
    "x_min = np.zeros((1, 2))\n",
    "x_max = np.ones((1, 2))\n",
    "x_box = np.concatenate([x_min[:, None], x_max[:, None]], axis=1)\n",
    "\n",
    "# Get lower and upper bounds\n",
    "lower_bound, upper_bound = decomon_model.predict_on_single_batch_np(\n",
    "    x_box\n",
    ")  # more efficient than predict on very small batch\n",
    "\n",
    "print(f\"lower bound: {lower_bound}\")\n",
    "print(f\"upper bound: {upper_bound}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26f590c-408a-42df-b104-a76e64a82c6b",
   "metadata": {},
   "source": [
    "Compare with empirical bounds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70079724-8050-4d06-9c98-b8d32778ec6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_input = K.convert_to_tensor(np.random.random((100, 2)))\n",
    "keras_output = K.convert_to_numpy(model(keras_input))\n",
    "lower_empirical = np.min(keras_output, axis=0)\n",
    "upper_empirical = np.max(keras_output, axis=0)\n",
    "\n",
    "print(f\"empirical lower bound: {lower_empirical}\")\n",
    "print(f\"empirical upper bound: {upper_empirical}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b72d24-8b6f-4c2c-8d74-e42d971af06c",
   "metadata": {},
   "source": [
    "We should have (and the tightest, the best the approximation):\n",
    "\n",
    "    lower_bounds <= lower_empirical  , upper_empirical <= upper_bound\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5326dd7e-62fd-4311-81a2-ff9ab714345c",
   "metadata": {},
   "source": [
    "That's all folks!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
