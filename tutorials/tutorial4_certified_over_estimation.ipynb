{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "353d4924",
   "metadata": {},
   "source": [
    " # DECOMON tutorial #4 \n",
    "\n",
    "_**Overestimation with formal guarantee for Braking Distance Estimation**_\n",
    "\n",
    "<decomonlinks>\n",
    "<p align=\"center\">\n",
    "  <img src=\"data/decomon.jpg\" alt=\"Decomon!\" width=\"100\">\n",
    "</p>\n",
    "\n",
    "\n",
    "\n",
    "- &#x1F4DA; <a href=\"https://airbus.github.io/decomon\"> Documentation </a>\n",
    "- <a href=\"https://github.com/airbus/decomon\"> Github </a>\n",
    "- <a href=\"https://airbus.github.io/decomon/main/tutorials.html \"> Tutorials </a>\n",
    "    \n",
    "_Author: [Melanie DUCOFFE](https://fr.linkedin.com/in/m%C3%A9lanie-ducoffe-bbb53165)_\n",
    "</decomonlinks>\n",
    "\n",
    "In recent years, we have seen the emergence of safety-related properties for regression tasks in many industries. For example, numerical models have been developed to approximate the physical phenomena inherent in their systems. Since these models are based on physical equations, the relevance of which is affirmed by scientific experts, their qualifications are carried out without any problems.\n",
    " However, as their computational costs and execution time prevent us from embedding them, the use of these numerical models in the aeronautical domain remains mainly limited to the development and design phase of the aircraft. Thanks to the current success of deep neural networks, previous works have already studied neural network-based surrogates for the approximation of numerical models. Nevertheless, these surrogates have additional safety properties that need to be demonstrated to certification authorities. In this blog post, we will examine a specification that arises for a neural network used for take-off distance estimation which is the over-estimation of the simulation model. We will explore how to address them with _**decomon**_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fa7435",
   "metadata": {},
   "source": [
    "Embedding simulation models developed during the design of a platform opens a lot of potential \n",
    "new functionalities but requires additional certification. \n",
    "Usually, these models require too much computing power, take too much time to run \n",
    "so we need to build an approximation of these models that can be compatible with operational constraints,\n",
    "hardware constraints, and real-time constraints. \n",
    "Also, we need to prove that the decisions made by the system using the surrogate model \n",
    "instead of the reference one will be **safe**. **The confidence in its safety has to be demonstrated \n",
    "to certification authorities**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c599e89e",
   "metadata": {},
   "source": [
    "- When running this notebook on Colab, we need to install *decomon* if on Colab. \n",
    "- If you run this notebook locally, do it inside the environment in which you [installed *decomon*](https://airbus.github.io/decomon/main/install.html) as well as [scipy](https://scipy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a603d78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On Colab: install the library\n",
    "on_colab = \"google.colab\" in str(get_ipython())\n",
    "if on_colab:\n",
    "    import sys  # noqa: avoid having this import removed by pycln\n",
    "\n",
    "    # install dev version for dev doc, or release version for release doc\n",
    "    !{sys.executable} -m pip install -U pip\n",
    "    !{sys.executable} -m pip install git+https://github.com/airbus/decomon@main#egg=decomon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f17f7b",
   "metadata": {},
   "source": [
    "## Landing Distance Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac59c5f",
   "metadata": {},
   "source": [
    "In many cases, **safety can be ensured by systematically over estimating** the reference model. An intuitive example\n",
    "is a surrogate that would predict the landing distance of an aircraft. Under-estimating the reference  distance  could  lead  to  an  avoidable  overrun. On  the contrary, over-estimating the reference distance may lead to unnecessary turn-around maneuvers when the landing was indeed safely possible.\n",
    "\n",
    "**Based on this reasonning we can define safety as the following requirement: the neural network will over-estimate the reference model on any point**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"./usecase/img/safe_surrogate.png\" alt=\"SafeOverestimation!\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "In this notebook, we will demonstrate how to _**train and assert with that a neural network is a safe surrogate**_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e227ae3",
   "metadata": {},
   "source": [
    "### UseCase Description\n",
    "\n",
    "The [Cessna 172 Skyhawk](https://en.wikipedia.org/wiki/Cessna_172) is an American four-seat, single-engine, high wing, fixed-wing aircraft made by the Cessna Aircraft Company. Measured by its longevity and popularity, the Cessna 172 is the most successful aircraft in history. Cessna delivered the first production model in 1956, and as of 2015, the company and its partners had built more than 44,000 units.The aircraft remains in production today.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"usecase/img/cesna_c_172.jpeg\" alt=\"SafeOverestimation!\" width=\"400\">\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f8b227",
   "metadata": {},
   "source": [
    "## Hands on ! Getting the data and training a braking distance estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da211b90",
   "metadata": {},
   "source": [
    "We will assess the over-estimation guanratees we can obtain for landing estimation on a toy usecase (considering only  pressure altitude and temperature).\n",
    "You can also build your own simulator using free online calculator for CESSNA C172 (source Flight Performance App)\n",
    "\n",
    "### Downloads\n",
    "\n",
    "the simulation code is available here as <a href='https://github.com/ducoffeM/safety_braking_distance_estimation/blob/main/cesna_simulation.py'>cesna_simulation.py</a> and should be stored in a subdirectory *safety_braking_distance_estimation*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f323ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/ducoffeM/safety_braking_distance_estimation.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e0e45c",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b410e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as patches\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from safety_braking_distance_estimation.cesna_simulation import cesna_landing\n",
    "\n",
    "%matplotlib inline\n",
    "import ipywidgets as widgets\n",
    "import numpy as np\n",
    "import tensorflow.keras.backend as K\n",
    "from ipywidgets import interact\n",
    "from scipy import stats\n",
    "from tensorflow.keras.layers import Activation, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from decomon import get_lower_box\n",
    "from decomon.models import clone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a980fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_temp = 0  # celsius\n",
    "MAX_temp = 40\n",
    "MIN_alt = 0\n",
    "MAX_alt = 4000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa60e7ed",
   "metadata": {},
   "source": [
    "### Data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282f3cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(N, MIN=[MIN_temp, MIN_alt], MAX=[MAX_temp, MAX_alt]):\n",
    "    alpha_temp = np.array([np.random.rand() for _ in range(N)])\n",
    "    alpha_alt = np.array([np.random.rand() for _ in range(N)])\n",
    "\n",
    "    X = np.zeros((N, 2))\n",
    "    X[:, 0] = alpha_temp * MIN[0] + (1 - alpha_temp) * MAX[0]\n",
    "    X[:, 1] = alpha_alt * MIN[1] + (1 - alpha_alt) * MAX[1]\n",
    "\n",
    "    Y = [cesna_landing(X[i, 0], X[i, 1]) for i in range(N)]\n",
    "\n",
    "    return X, np.array(Y)  # samples, associated landing distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80955d74",
   "metadata": {},
   "source": [
    "We first generate randomly three datasets: training, validation and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2686745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate training, validation and test dataset\n",
    "X_train, y_train = generate_dataset(10000)\n",
    "X_valid, y_valid = generate_dataset(1000)\n",
    "X_test, y_test = generate_dataset(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8f89ef",
   "metadata": {},
   "source": [
    "We normalize the data according to the mean and variance on the train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b52bc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "MEAN_x = np.mean(X_train, 0)[None]\n",
    "STD_x = np.std(X_train, 0)[None]\n",
    "\n",
    "X_train_ = (X_train - MEAN_x) / STD_x\n",
    "X_valid_ = (X_valid - MEAN_x) / STD_x\n",
    "X_test_ = (X_test - MEAN_x) / STD_x\n",
    "\n",
    "MEAN_y = np.mean(y_train, 0)\n",
    "STD_y = np.std(y_train, 0)\n",
    "\n",
    "y_train_ = (y_train - MEAN_y) / STD_y\n",
    "y_valid_ = (y_valid - MEAN_y) / STD_y\n",
    "y_test_ = (y_test - MEAN_y) / STD_y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d972167",
   "metadata": {},
   "source": [
    "We use the [Tensorflow/Keras library](https://www.tensorflow.org/) to encode our Neural Network that will predict the landing distance based on the pressure altitude and the temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac102a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a feedforward neural network\n",
    "layers = [Dense(50, input_dim=2), Activation(\"softsign\"), Dense(50), Activation(\"softsign\"), Dense(1)]\n",
    "model = Sequential(layers)\n",
    "model.compile(\"adam\", \"mse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e502fc",
   "metadata": {},
   "source": [
    "you can visualize the architecture of the model using the method summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5533eee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a979e655",
   "metadata": {},
   "source": [
    "we train the neural network on the normalised dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d27c43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train_, y_train_, batch_size=32, epochs=10, validation_data=(X_valid_, y_valid_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf02a7f",
   "metadata": {},
   "source": [
    "we evaluate the neural network's performance on the normalised test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69c450e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# empirical assessment of the model\n",
    "y_pred_test = model.predict(X_test_)\n",
    "model.evaluate(X_test_, y_test_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295588e0",
   "metadata": {},
   "source": [
    "## Empirical evaluation of Over estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6089476",
   "metadata": {},
   "source": [
    "\n",
    "Even if the surrogate model's predictions are close to the test distance, we may observe on the test dataset that there is still underestimation.\n",
    "\n",
    "Two naive solutions are possible: retraining the model with an assymetric loss that promote overestimation or adding a *shift*, meaning an additive penalty to ensure ovestimation: $$f_{shift}(\\cdot)= f(\\cdot) + \\text{shift}$$\n",
    "\n",
    "We observe that our surrogate is not **safe** at all: there are many points that are under-estimated by our **surrogate**. One solution to improve our surrogate is *to minimize the number of times it under-estimates a sample on the test set*. This comes to adding a positive shift to our model.\n",
    "\n",
    "Note that the shift value should be independent of the test set and calibrated on an independent data set as proposed in [Ducoffe et al.](https://ceur-ws.org/Vol-2560/paper11.pdf). To simplify the concept we let the user choose it manually in the interface."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261173d3",
   "metadata": {},
   "source": [
    "We store the bias of the last neural network's layer to modify its value according to the shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a804d839",
   "metadata": {},
   "outputs": [],
   "source": [
    "bias = model.layers[-1].bias.value()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f279b0b6",
   "metadata": {},
   "source": [
    "We define an automatic function: *pick shift*. It plots two information:\n",
    "\n",
    "        - the empirical distribution of _**underestimation**_ according to the predicted distance: wich predicted distance are potentially unsafe\n",
    "        - the density (histogram) of test samples according to the difference between the targeted distance and the predicted distance\n",
    "        \n",
    "The shift of the model is reset to 0 at the end of the method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aea8e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_shift(shift=0.0):\n",
    "    # normalize the shift\n",
    "    K.set_value(model.layers[-1].bias, bias + shift)\n",
    "    y_pred_test_ = model.predict(X_test_, verbose=0) * STD_y + MEAN_y\n",
    "    # modify bias\n",
    "    index_over = np.where(y_pred_test_[:, 0] >= y_test)[0]\n",
    "    index_under = np.where(y_pred_test_[:, 0] < y_test)[0]\n",
    "\n",
    "    n_over = 1.0 * max(len(index_over), 1)\n",
    "    n_under = 1.0 * max(len(index_under), 1)\n",
    "\n",
    "    hist_over_0, bin_over_0 = np.histogram(y_test[index_over] - y_pred_test_[index_over, 0], 100)\n",
    "    hist_under_0, bin_under_0 = np.histogram(y_test[index_under] - y_pred_test_[index_under, 0], 10)\n",
    "\n",
    "    kde_over = stats.gaussian_kde(y_pred_test_[index_over, 0])\n",
    "    xx = np.linspace(y_test.min(), y_test.max(), 100)\n",
    "    K.set_value(model.layers[-1].bias, bias)\n",
    "\n",
    "    n_rows = 1\n",
    "    n_cols = 2\n",
    "    fig, axs = plt.subplots(n_rows, n_cols)\n",
    "\n",
    "    _ = axs[0].plot(xx, kde_over(xx), color=\"green\", alpha=0.5)\n",
    "    legend = [\"over\"]\n",
    "    if len(index_under) > 1:\n",
    "        kde_under = stats.gaussian_kde(y_pred_test_[index_under, 0])\n",
    "        _ = axs[0].plot(xx, kde_under(xx), color=\"red\", alpha=0.5)\n",
    "        legend.append(\"under\")\n",
    "    axs[0].legend(legend)\n",
    "    axs[0].set_xlabel(\"predicted distance\")\n",
    "    axs[0].set_ylabel(\"proba(test samples)\")\n",
    "\n",
    "    _ = axs[1].hist(bin_over_0[:-1], bins=bin_over_0, weights=hist_over_0, color=\"green\", alpha=0.5)\n",
    "    if len(index_under):\n",
    "        _ = axs[1].hist(bin_under_0[:-1], bins=bin_under_0, weights=hist_under_0, color=\"red\", alpha=0.5)\n",
    "        axs[1].legend([\"overestimation\"])\n",
    "    else:\n",
    "        axs[1].legend([\"overestimation\", \"underestimation\"])\n",
    "    axs[1].set_xlabel(\"true distance - predicted distance\")\n",
    "\n",
    "    if len(index_under):\n",
    "        fig.suptitle(\"UNSAFE 😲: underestimation on the test set with shift={:.2f}\".format(shift))\n",
    "    else:\n",
    "        fig.suptitle(\"SAFE 😊 ?: no underestimation on the test set with shift={:.2f}\".format(shift))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381ea5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "interact(\n",
    "    pick_shift,\n",
    "    shift=widgets.FloatSlider(value=0, min=0, max=0.1, step=0.001, continuous_update=False),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18502f4b",
   "metadata": {},
   "source": [
    "## DECOMON: Formal method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a027e958",
   "metadata": {},
   "source": [
    "Next we illustrate how to get a full demonstration of over-estimation thanks to decomon. You can see the [record](https://www.youtube.com/watch?v=Tq-QlRcUf8Q) of the presentation to have a better understanding of the underlying techniques.\n",
    "\n",
    "Many surrogate models have some specificities: their monotonicity of their output given their inputs.\n",
    "This information is highly valuable to bound locally the output of the reference function. Such bounds can be coupled\n",
    "with formal methods on the neural networks to over-approximate the worst case difference between the reference function and the neural network. Thanks to monotonicity, we can build a set of [Majoring Point](https://arxiv.org/abs/2101.11717) ❌\n",
    "on which the groundtruth distance can be used as a local upper bound for the distance.\t\n",
    "\n",
    "We can use this *local worst case upper bound* <span style=\"color:red\">---</span> with formal methods\n",
    "decomon can output a local upper bound <span style=\"color:orange\">___</span>. If this local upper bound is greater than the local worst case bound, \n",
    "it demonstrates formally that the surrogate model is safe, as it will always over-estimate the groundtruth function.\n",
    "\n",
    "We first start by building a piecewise constant over-approximation of our groundtruth function using Majoring Points (MP, ❌).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54762de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_1D = 10  # number of samples\n",
    "\n",
    "\n",
    "# sample for bounding\n",
    "index = 1\n",
    "# freeze the temperature\n",
    "\n",
    "alt = np.linspace(MIN_alt, MAX_alt, n_1D)\n",
    "\n",
    "# do linspace\n",
    "curve_samples = [generate_dataset(1, MIN=[MIN_temp, alt_i], MAX=[MIN_temp, alt_i]) for alt_i in alt]\n",
    "random_samples = generate_dataset(n_1D, MIN=[MIN_temp, MIN_alt], MAX=[MIN_temp, MAX_alt])\n",
    "x_1D = np.array([sample[0][0, index] for sample in curve_samples])\n",
    "y_1D = np.array([sample[1] for sample in curve_samples])\n",
    "index_sort = np.argsort(x_1D)\n",
    "x_1D = x_1D[index_sort]\n",
    "y_1D = y_1D[index_sort]\n",
    "\n",
    "X_rand_1D = random_samples[0][:, index]\n",
    "Y_rand_1D = random_samples[1]\n",
    "\n",
    "plt.scatter(X_rand_1D, Y_rand_1D, color=\"blue\", marker=\"^\", alpha=0.2)\n",
    "for i in range(n_1D - 1):\n",
    "    line_i = [x_1D[i], x_1D[i + 1]]\n",
    "    plt.plot(line_i, [y_1D[i + 1]] * 2, \"--\", c=\"red\")\n",
    "    if i == 0:\n",
    "        plt.scatter(x_1D, y_1D, color=\"red\", marker=\"x\")\n",
    "        plt.legend([\"random sample\", \"worst case distance\", \"grid (MP)\"])\n",
    "\n",
    "# plt.scatter(x_1D, y_1D, marker='x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d41575",
   "metadata": {},
   "outputs": [],
   "source": [
    "decomon_model = clone(model, method=\"crown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45eab17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formal_shift_1D(shift=0, granularity=10, emp_sampling=20):\n",
    "    # we normalize the shift\n",
    "    K.set_value(model.layers[-1].bias, bias + shift)\n",
    "    n_1D = granularity  # number of samples\n",
    "\n",
    "    # sample for bounding\n",
    "    index = 1\n",
    "    # freeze the temperature\n",
    "    alt = np.linspace(MIN_alt, MAX_alt, n_1D)\n",
    "\n",
    "    # do linspace\n",
    "    curve_samples = [generate_dataset(1, MIN=[MIN_temp, alt_i], MAX=[MIN_temp, alt_i]) for alt_i in alt]\n",
    "    random_samples = generate_dataset(n_1D, MIN=[MIN_temp, MIN_alt], MAX=[MIN_temp, MAX_alt])\n",
    "\n",
    "    x_1D = np.array([sample[0][0, index] for sample in curve_samples])\n",
    "    X_rand_1D = random_samples[0][:, index]\n",
    "\n",
    "    y_1D = np.array([sample[1] for sample in curve_samples])\n",
    "    index_sort = np.argsort(x_1D)\n",
    "    x_1D = x_1D[index_sort]\n",
    "    y_1D = y_1D[index_sort]\n",
    "\n",
    "    Y_rand_1D = random_samples[1]\n",
    "    Y_pred_1D = model.predict((random_samples[0] - MEAN_x) / STD_x)[:, 0] * STD_y + MEAN_y\n",
    "\n",
    "    z_1D = (np.array([sample[0][0, :] for sample in curve_samples]) - MEAN_x) / STD_x\n",
    "\n",
    "    MIN_y = y_1D[0] - 5\n",
    "    MAX_y = y_1D[-1] + 5\n",
    "\n",
    "    plt.scatter(X_rand_1D, Y_rand_1D, color=\"blue\", marker=\"^\", alpha=0.2)\n",
    "    plt.scatter(X_rand_1D, Y_rand_1D, color=\"purple\", marker=\"v\", alpha=0.7)\n",
    "    count_unsafe = 0\n",
    "\n",
    "    for i in range(n_1D - 1):\n",
    "        line_i = [x_1D[i], x_1D[i + 1]]\n",
    "        plt.plot(line_i, [y_1D[i + 1]] * 2, \"--\", c=\"red\")\n",
    "\n",
    "        x_min_i = z_1D[i][None]\n",
    "        x_max_i = z_1D[i + 1][None]\n",
    "        upper_i = get_lower_box(decomon_model, x_min=x_min_i, x_max=x_max_i)[0, 0] * STD_y + MEAN_y\n",
    "\n",
    "        if upper_i > MAX_y:\n",
    "            MAX_y = upper_i + 5\n",
    "        if upper_i < MIN_y:\n",
    "            MIN_y = upper_i - 5\n",
    "\n",
    "        if upper_i >= y_1D[i + 1]:\n",
    "            plt.plot(line_i, [upper_i] * 2, \"-\", c=\"green\")\n",
    "        else:\n",
    "            count_unsafe += 1\n",
    "            plt.plot(line_i, [upper_i] * 2, \"-\", c=\"orange\")\n",
    "\n",
    "        plt.scatter(x_1D, y_1D, marker=\"x\", c=\"red\")\n",
    "        if i == 0:\n",
    "            plt.legend([\"GT sample\", \"prediction\", \"worst case GT distance\", \"formal lower prediction\", \"grid (MP)\"])\n",
    "\n",
    "    plt.ylim([MIN_y, MAX_y])\n",
    "\n",
    "    if count_unsafe:\n",
    "        plt.title(\"UNSAFE 😲 ?: potential underestimation with shift={:.2f}\".format(shift))\n",
    "    else:\n",
    "        plt.title(\"SAFE 😊 !: no underestimation with shift={:.2f}\".format(shift))\n",
    "    K.set_value(model.layers[-1].bias, bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4812c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "interact(\n",
    "    formal_shift_1D,\n",
    "    shift=widgets.FloatSlider(value=0, min=0, max=0.8, step=0.001, continuous_update=False),\n",
    "    granularity=widgets.IntSlider(value=10, min=1, max=20, step=1, continuous_update=False),\n",
    "    emp_sampling=widgets.IntSlider(value=10, min=10, max=100, step=1, continuous_update=False),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a460b4a",
   "metadata": {},
   "source": [
    "This proof can be easily extended to any dimension, at the cost of worst over-approximation cost (higher shift value).\n",
    "We split the space in boxes for various range of pressure altitude and temperature and run our method on each box.\n",
    "The color of the box will be <span style=\"color:orange\">orange</span> if we cannot prove formally overestimation by the shifted neural network and <span style=\"color:green\">green</span> otherwhise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8e811e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formal_shift_2D(shift, grid_alt=1, grid_temp=1):\n",
    "    K.set_value(model.layers[-1].bias, bias + shift)\n",
    "    n_1D_alt = grid_alt  # number of split along the 'altitude' dimension\n",
    "    n_1D_temp = grid_temp  # number of split along the 'temperature' dimension\n",
    "    # sample for bounding\n",
    "    alt = np.linspace(MIN_alt, MAX_alt, n_1D_alt)\n",
    "    temp = np.linspace(MIN_temp, MAX_temp, n_1D_temp)\n",
    "\n",
    "    # Create figure and axes\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    ax.set_xticks(alt)\n",
    "    ax.set_yticks(temp)\n",
    "    count_unsafe = 0\n",
    "    error_formal = []\n",
    "\n",
    "    for i, alt_i in enumerate(alt[:-1]):\n",
    "        for j, temp_j in enumerate(temp[:-1]):\n",
    "            d_x_i = alt[i + 1] - alt[i]\n",
    "            d_y_j = temp[j + 1] - temp[j]\n",
    "\n",
    "            x_max_i = alt[i + 1]\n",
    "            x_max_j = temp[j + 1]\n",
    "            _, y_max_ij = generate_dataset(1, MIN=[x_max_i, x_max_j], MAX=[x_max_i, x_max_j])\n",
    "            lower_ij = (\n",
    "                get_lower_box(decomon_model, x_min=np.array([temp[j], alt[i]]), x_max=np.array([x_max_j, x_max_i]))[\n",
    "                    0, 0\n",
    "                ]\n",
    "                * STD_y\n",
    "                + MEAN_y\n",
    "            )\n",
    "\n",
    "            if lower_ij >= y_max_ij:\n",
    "                color = \"green\"\n",
    "            else:\n",
    "                error_formal.append((lower_ij - y_max_ij))\n",
    "                count_unsafe = 1\n",
    "                color = \"orange\"\n",
    "            rect = patches.Rectangle((alt_i, temp_j), d_x_i, d_y_j, color=color)\n",
    "            ax.add_patch(rect)\n",
    "    if count_unsafe:\n",
    "        plt.title(\"UNSAFE 😲 ?: potential underestimation with shift={:.2f}\".format(shift))\n",
    "    else:\n",
    "        plt.title(\"SAFE 😊 !: no underestimation with shift={:.2f}\".format(shift))\n",
    "    K.set_value(model.layers[-1].bias, bias)\n",
    "\n",
    "    plt.xlim([MIN_alt, MAX_alt])\n",
    "    plt.ylim(MIN_temp, MAX_temp)\n",
    "    plt.grid()\n",
    "    plt.ylabel(\"temperature\")\n",
    "    plt.xlabel(\"pressure altitude\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da11b2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "interact(\n",
    "    formal_shift_2D,\n",
    "    shift=widgets.FloatSlider(value=0, min=0, max=10, step=0.001, continuous_update=False),\n",
    "    grid_alt=widgets.IntSlider(value=10, min=1, max=20, step=1, continuous_update=False),\n",
    "    grid_temp=widgets.IntSlider(value=10, min=1, max=20, step=1, continuous_update=False),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
